{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f185678f",
   "metadata": {},
   "source": [
    "# Facial Recognition/Verification Main Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd663b9",
   "metadata": {},
   "source": [
    "#### This is the second/main model for our facial recognition project. This model like the previous mirrors the structure of a VGG model. The key difference is that the model was trained on 2622 identities instead of just the current number of users inside of the  authorized user face dataset created by OpenCV. This approach solves the main problems with the previous model:\n",
    "#### a) This model does not have to be re-trained anytime a new user is added to the authorized user face dataset. b) The model has more identities to work with. Meaning the model does not have to try to assign each face to only three options. This helped out a lot because while the first model was very accurate at detecting the authorized user faces... it struggled with detected unauthorized users. Even though, we attempted to add a threshold to the first model like the professor suggested. It still struggled with predicting unknown people.\n",
    "#### This model worked to solve both of those issues, and added an proof of concept behind the security aspect of the project. In the model here, a encrypted file will try to be read. This text file will be unreadable. The webcamera will open up and try to detect authorized users. If authorized users are detected, then the file decodes. If unauthorized users are detected, the the file remains encoded. *** The web application will do something slightly different. It will display the user information instead, as we had issues getting the text file to display in a meaningful way in the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0625a",
   "metadata": {},
   "source": [
    "### References:\n",
    "VGG Code Reference: https://github.com/serengil/tensorflow-101/blob/master/python/deep-face-real-time.py\n",
    "\n",
    "VGG Model Structure Reference: https://neurohive.io/en/popular-networks/vgg16/\n",
    "\n",
    "Description: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49625960",
   "metadata": {},
   "source": [
    "### Libraries Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0032b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used for OpenCv and path creation/reading\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "# Keras libraries necessary for the model creation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Libraries for preprocessing the data\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# Importing numpy to work with arrays, Fernet for decrypting text file\n",
    "# Spatical for finding similarity between images\n",
    "import numpy as np\n",
    "from cryptography.fernet import Fernet\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f14936",
   "metadata": {},
   "source": [
    "### Model Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae529c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a CNN \n",
    "# Download data from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "# A VGG16 Model typically has the following structure: https://neurohive.io/en/popular-networks/vgg16/\n",
    "\n",
    "# A sequential model is defined here for the faces from VGG\n",
    "model = Sequential()\n",
    "\n",
    "# The image shape is equal to the image shapes of the faces gathered from OpenCV.\n",
    "image_shapes = (224, 224, 3)\n",
    "\n",
    "# These first five blocks mirror that of a standard VGG model with 16 weight layers. The blocks apply\n",
    "# a series of 2D convolution layers progressively expanding the number of filters until it gets to 512.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=image_shapes))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Like usual VGG models, the last block is where the refining is done for the dataset the model\n",
    "# is being applied on. After going through the last couple of convolutional and dropout layers to\n",
    "# condense the number of parameters, the model is then flattened so that it can easily pick which\n",
    "# face to assign where.\n",
    "model.add(Conv2D(4096, kernel_size=(7, 7), activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(4096, kernel_size=(1, 1), activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(2622, kernel_size=(1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# To save time, the VGG weights are loaded here.\n",
    "model.load_weights(r'C:\\Users\\jpasz\\vgg_face_weights.h5')\n",
    "\n",
    "# Generate model with input and output\n",
    "model = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81945619",
   "metadata": {},
   "source": [
    "### The Text File is Encrypted:\n",
    "\n",
    "#### This block of code is here to demonstrate that the file that the user wants to access is currently unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e1b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted file:  gAAAAABidtzysaDx0JJxpdO3DEfu86DOER1Wr044NbpS_WeaTAybOYWWH09Yn75nR6Tj1yQTsxXk3HDEEvjKsiNTSgLfmXu-ImrG49VkygMHbAMWKIerWAJmjUx6up0FnjQ0FfZ7SLsNg-0BGVn4qRSGuNnUl0XqqLQhigIAG2ZIFXRpim-hPja5Wp4JjQ1unIeCtx4FRRgV \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The text file is read to grab the encrypted data, and print each encrypted line below.\n",
    "with open(r'FR Encryption Example\\FR Encryption Example.txt', 'rb') as read_file:\n",
    "    encrypted_data = read_file.read()\n",
    "print('Encrypted file: ', encrypted_data.decode(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909e0c5",
   "metadata": {},
   "source": [
    "### New Faces Model Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f645f",
   "metadata": {},
   "source": [
    "#### The vgg cnn model assigns new images captured by OpenCV FR Dataset Generation.ipynb. We can say the images are train datasets. The output from the cnn is saved in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1294aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a file name and dictionary\n",
    "new_pictures_folder = \"New Faces\"\n",
    "new_pictures = dict()\n",
    "\n",
    "# Iterate a folder (\"New Faces\") to folder (\"User name\") to user's 150 face images\n",
    "# String beocomes file path by listdir\n",
    "for folder_name in listdir(new_pictures_folder):\n",
    "    if folder_name != '.DS_Store':  # for mac users\n",
    "        \n",
    "        folder_path = new_pictures_folder+'/'+folder_name  # create a path for the next folder to iterate\n",
    "        \n",
    "        pred = []  # initialize and reset a list\n",
    "        \n",
    "        for file_name in listdir(folder_path):\n",
    "            file_name = folder_path+'/'+file_name  # create a path for the files\n",
    "            face_img = load_img(file_name, target_size=(224, 224))  # load images by the path\n",
    "            face_array = img_to_array(face_img)\n",
    "            face_expanded = np.expand_dims(face_array, axis=0)\n",
    "            face_preprocessed = preprocess_input(face_expanded)\n",
    "            pred.append(model.predict(face_preprocessed)[0,:])  # get values from each images by model.predict\n",
    "            \n",
    "        new_pictures[folder_name] = pred  # assign the list to dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e57d1",
   "metadata": {},
   "source": [
    "#### The saved values in the dictionary is compared with the captured images extracted from real time video. If a cosine similarity between values in the dictionary and the values from webcam images are very similar, it is considered as the same face and displays the user's name. Otherwise the detected face is shown as a unauthorized user. A count is kept for the number of times an authorized/unauthorized user has to be detected for the webcamera to close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba014e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of variables are defined here. Color is the rgb value for black. user_face_matches\n",
    "# and unauthorized_matches are counters for the results of the face verification.\n",
    "color = (0, 0, 0)\n",
    "user_face_matches = 0\n",
    "unauthorized_matches = 0\n",
    "\n",
    "# The webcamera is opened up with OpenCV.\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# This while loop will try to match detected faces with authorized users. If it succeeds, then the\n",
    "# counter goes up. If the detected face is an unauthorized user a different counter goes up. Once a\n",
    "# threshold of counters is met, the camera is closed and the file will be given a flag to either stay\n",
    "# encoded or decode.\n",
    "while((user_face_matches < 20) and (unauthorized_matches < 10)):\n",
    "    result, image = camera.read()  # get a image from webcam capture\n",
    "    # If the image capture is sucessful, this if statement is run.\n",
    "    if result:\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(image, 1.1, 4)  # Find faces in the captured image\n",
    "        for (x,y,w,h) in faces:  # x, y represents initial posions in a graph. w is width and h is height\n",
    "            # If the width of the face is a relatively big, then it will be scanned with the model.\n",
    "            if w > 75: \n",
    "                user_detected = False\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                detected_face = image[int(y):int(y+h), int(x):int(x+w)] # crop detected face\n",
    "                detected_face = cv2.resize(detected_face, (224, 224)) # resize to 224x224\n",
    "                face_array = img_to_array(detected_face)\n",
    "                face_expanded = np.expand_dims(face_array, axis=0)\n",
    "                face_preprocessed = preprocess_input(face_expanded)\n",
    "                \n",
    "                # Assign the image extracted from the webcam\n",
    "                captured_representation = model.predict(face_preprocessed)\n",
    "                for k in new_pictures:  # k is a key of dict which is user's name\n",
    "                    folder = new_pictures[k]\n",
    "                    for i in folder:\n",
    "                        cosine_distance = spatial.distance.cosine(i, captured_representation)\n",
    "                        cosine_similarity = 1 - cosine_distance\n",
    "                        \n",
    "                        # If cosine similarity is greater than 0.8, then consider that detected\n",
    "                        # face as the user in folder k, and increment the counter.\n",
    "                        if(cosine_similarity > 0.80): \n",
    "                            cv2.putText(image, k, (int(x+w+5), int(y-5)), cv2.FONT_HERSHEY_TRIPLEX, 1, color, 1)\n",
    "                            user_detected = True\n",
    "                            user_face_matches = user_face_matches + 1\n",
    "                            break\n",
    "                            \n",
    "                # If the detected image is not in user's face dataset, then display unauthorized\n",
    "                # and increment the unauthorized counter.\n",
    "                if(user_detected == False): \n",
    "                    cv2.putText(image, 'Unauthorized', (int(x+w+5), int(y-5)), cv2.FONT_HERSHEY_TRIPLEX, 1, color, 1)\n",
    "                    unauthorized_matches = unauthorized_matches + 1\n",
    "    # Displays the webcamera to the user.               \n",
    "    cv2.imshow('Facial Recognition',image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # press q to quit\n",
    "        break\n",
    "\n",
    "# These two lines shut down OpenCV\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# If the counter in authorized users is 20, then give the flag to decrypt the text file.\n",
    "# Otherwise, remain encrypted.\n",
    "encrypt_file = False\n",
    "if (user_face_matches == 20):\n",
    "    encrypt_file = True\n",
    "else:\n",
    "    encrypt_file = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc5f34",
   "metadata": {},
   "source": [
    "### Is the Text File Encrypted or Decrypted?:\n",
    "\n",
    "#### The file now either remains encoded, or is decoded if an authorized user is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62571ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted file:  Hi! This is an example of decrypting data with the use of the facial recognition program.\n"
     ]
    }
   ],
   "source": [
    "if (encrypt_file == True):\n",
    "    # The key file is read to grab the key for decoding.\n",
    "    with open(r'FR Encryption Example\\Facial_Recognition_Key.key', 'rb') as key_file:\n",
    "        fr_key = key_file.read()\n",
    "    # The text file is read to grab the encrypted data.\n",
    "    with open(r'FR Encryption Example\\FR Encryption Example.txt', 'rb') as read_file:\n",
    "        encrypted_data = read_file.read()\n",
    "\n",
    "    # Fernet grabs the key and decrypts the encrypted text file\n",
    "    fernet = Fernet(fr_key)\n",
    "    decrypted_document = fernet.decrypt(encrypted_data)\n",
    "    print('Decrypted file: ', decrypted_document.decode())\n",
    "    \n",
    "elif(encrypt_file == False):\n",
    "    # The text file is read to grab the encrypted data.\n",
    "    with open(r'FR Encryption Example\\FR Encryption Example.txt', 'rb') as read_file:\n",
    "        encrypted_data = read_file.read()\n",
    "    print('Encrypted file: ', encrypted_data.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04c1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_tensorflow",
   "language": "python",
   "name": "gpu_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
