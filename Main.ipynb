{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e0625a",
   "metadata": {},
   "source": [
    "### Please refer to https://github.com/serengil/tensorflow-101/blob/master/python/deep-face-real-time.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49625960",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0032b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf713647",
   "metadata": {},
   "source": [
    "RGB color code for texts. 0, 255, 0 is lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbaf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0,255,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d75b60",
   "metadata": {},
   "source": [
    "Create a classifier for face detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ed967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f14936",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64b1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\n",
    "# Devide the array by 127.5 and substract by 1\n",
    "# Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    img = img_to_array(image)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN \n",
    "# Download data from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "\n",
    "def loadVggFaceModel():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Load pretrained weights for VGGFace\n",
    "    from keras.models import model_from_json\n",
    "    model.load_weights('/Users/jake/Desktop/CS490/Project/vgg_face_weights.h5')\n",
    "    \n",
    "    # Generate model with input and output\n",
    "    vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "\n",
    "    return vgg_face_descriptor  # return the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81945619",
   "metadata": {},
   "source": [
    "### Real time face recognition\n",
    "Use the cnn model and assign new images captured by OpenCV FR Dataset Generation.ipynb. We can say the images are train datasets.   \n",
    "The output from the cnn is saved in dictionary. The saved values of dictionary is compared with images (test datasets) extracted from a real time video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e724795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model by function call\n",
    "model = loadVggFaceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1294aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 19:44:52.738673: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "new_pictures_folder = \"New Faces\"\n",
    "new_pictures = dict()\n",
    "\n",
    "for folder_name in listdir(new_pictures_folder):\n",
    "    if folder_name != '.DS_Store':  # for mac users\n",
    "        folder_path = new_pictures_folder+'/'+folder_name\n",
    "        pred = []\n",
    "        for file_name in listdir(folder_path):\n",
    "            file_name = folder_path+'/'+file_name\n",
    "            face_img = load_img(file_name, target_size=(224, 224))\n",
    "            pred.append(model.predict(preprocess_image(face_img))[0,:])\n",
    "            \n",
    "        new_pictures[folder_name] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e963a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba014e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #webcam\n",
    "\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        if w > 130: \n",
    "\n",
    "            detected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "            detected_face = cv2.resize(detected_face, (224, 224)) #resize to 224x224\n",
    "\n",
    "            img_pixels = preprocess_image(detected_face)\n",
    "            \n",
    "            # assign test data\n",
    "            captured_representation = model.predict(img_pixels)[0,:]\n",
    "\n",
    "            found = 0\n",
    "            for k in new_pictures:\n",
    "                folder = new_pictures[k]\n",
    "                for i in folder:\n",
    "                    similarity = findCosineSimilarity(i, captured_representation)\n",
    "                    if(similarity < 0.30):\n",
    "                        cv2.putText(img, k, (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                    \n",
    "                        found = 1\n",
    "                        break\n",
    "\n",
    "            #connect face and text\n",
    "            cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
    "            cv2.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
    "\n",
    "            if(found == 0): #if found image is not in employee database\n",
    "                cv2.putText(img, 'unknown', (int(x+w+15), int(y-12)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs490_python_projects",
   "language": "python",
   "name": "cs490_python_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
